{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Breakdown\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanics\n",
    "\n",
    "Fill out this notebook, save it, and **submit it electronically as described below.**\n",
    "\n",
    "On a DICE environment, open the terminal, navigate to the location of this notebook, and submit this notebook file using the following command:\n",
    "\n",
    "`submit iaml cw2 09_Assignment_4.ipynb`\n",
    "\n",
    "What actually happens in the background is that your file is placed in a folder available to markers. If you submit a file with the same name into the same location, **it will *overwrite* your previous submission**. You can check the status of your submissions with the `show_submissions` command.\n",
    "\n",
    "**Distance Learners:** To copy your work up to DICE (such that you can use the `submit` command) you can use `scp` or `rsync` (you may need to install these yourself). You can copy files up using `student.ssh.inf.ed.ac.uk`, then ssh in to submit, e.g. (in a unix terminal):\n",
    "```\n",
    "filename=09_Assignment_4.ipynb\n",
    "local_scp_filepath=~/git/iaml2017/${filename}\n",
    "UUN=s0816700\n",
    "server_address=student.ssh.inf.ed.ac.uk\n",
    "scp -r ${local_scp_filepath} ${UUN}@${server_address}:${filename}\n",
    "# rsync -rl ${local_scp_filepath} ${UUN}@${server_address}:${filename}\n",
    "ssh ${UUN}@${server_address}\n",
    "ssh student.login\n",
    "submit iaml cw1 09_Assignment_4.ipynb\n",
    "```\n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics MSc Degree Guide is that normally you will not be allowed to submit coursework late. See http://www.inf.ed.ac.uk/teaching/years/msc/courseguide10.html#exam for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you should NOT borrow actual text or code from other students. We ask that you provide a list of the people who you've had discussions with (if any).\n",
    "\n",
    "**Resubmission:** If you submit your file again, the previous submission is **overwritten**. We will mark the version that is in the submission folder at the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "1. You *MUST* have your environment set up as in the [README](https://github.com/JamesOwers/iaml2017) and you *must activate this environment before running this notebook*:\n",
    "```\n",
    "source activate iaml\n",
    "cd iaml_2017\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers.\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (the `datasets` directory is adjacent to this file).\n",
    "\n",
    "1. **IMPORTANT:** Keep your answers brief and concise. Most written questions can be answered with 2-3 lines of explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this assignment you are asked to import all the packages and modules you will need. Include all required imports and execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the datasets\n",
    "\n",
    "\n",
    "This assignment is based on two datasets:\n",
    "1. the 20 Newsgroups Dataset (you should recognise it from Assignment 1)\n",
    "2. the MNIST digits dataset\n",
    "\n",
    "### 20 Newsgroups\n",
    "\n",
    "For convenience, we repeat the description here. This dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware, comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale, soc.religion.christian). \n",
    "\n",
    "To save you time and to make the problem manageable with limited computational resources, we preprocessed the original dataset. We will use documents from only 5 out of the 20 newsgroups, which results in a 5-class problem. More specifically the 5 classes correspond to the following newsgroups: \n",
    "1. `alt.atheism`\n",
    "2. `comp.sys.ibm.pc.hardware`\n",
    "3. `comp.sys.mac.hardware`\n",
    "4. `rec.sport.baseball`\n",
    "5. `rec.sport.hockey `\n",
    "\n",
    "However, note here that classes 2-3 and 4-5 are rather closely related.\n",
    "\n",
    "**In contrast to Assignment 1**, we have opted to use tf-idf weights ([term frequency - inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf))\n",
    "for each word instead of the frequency counts. These weights represent the importance of a word to a\n",
    "document with respect to a collection of documents. The importance increases proportionally to the number\n",
    "of times a word appears in the document and decreases proportionally to the number of times the word\n",
    "appears in the whole corpus. \n",
    "\n",
    "Additionally we preprocess the data to include the most frequent 1000 words that are in greater than 2 documents, less than half of all documents, and that are not [stop words](https://en.wikipedia.org/wiki/Stop_words).\n",
    "\n",
    "We will perform all this preprocessing for you.\n",
    "\n",
    "\n",
    "### MNIST\n",
    "This MNIST Dataset is a collection handwritten digits. The samples are partitioned (nearly) evenly across the 10 different digit classes {0, 1, . . . , 9}. We use a preprocessed version for which the data are $8 \\times 8$ pixel images containing one digit each. For further details on how the digits are preprocessed, see the sklearn documentation. The images are grayscale, with each pixel taking values in {0, 1, . . . , 16}, where 0 corresponds to black (weakest intensity) and 16 corresponds to white (strongest intensity). Therefore, the dataset is a N Ã— 64\n",
    "dimensional matrix where each dimension corresponds to a pixel from the image and N is the number of\n",
    "images. \n",
    "\n",
    "Again, to save you time, we perfom the import for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering the 20 Newsgroups Data [50%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 1 we will cluster the observations in the 20 Newsgroups dataset using the k-means algorithm. Each row of the dataset represents a document with bag of words features. If we were not given the labels for each document (i.e. the newsgroup it came from), clustering could allow us to infer which documents should have the same label. Observing common words within each cluster may allow us to give meaning to these inferred labels too.\n",
    "\n",
    "First we'll import the data and fit and evaluate k-means with 5 cluster centres. Next, we will try and infer which cluster corresponds with which label. Finally, we will pretend we don't know the number of clusters there should be, as is the normal scenario with large unlabeled data, and investigate the effect of using a different number of cluster centres (i.e. varying `k`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.0 --- [0 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cells to import the data. It would be a good idea to understand the code but it's not strictly necessary (see the [sklearn documentation](http://scikit-learn.org/0.17/datasets/index.html#the-20-newsgroups-text-dataset)).\n",
    "\n",
    "*This may take a wee while as it will download the dataset*\n",
    "\n",
    "**Do not change any of the code in this question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cats = ['alt.atheism', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', \n",
    "        'rec.sport.baseball', 'rec.sport.hockey']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats, \n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=1000,\n",
    "                             min_df=2, stop_words='english', \n",
    "                             token_pattern='(?u)\\\\b[A-z]{2,}\\\\b')\n",
    "X_sparse = vectorizer.fit_transform(newsgroups_train.data)\n",
    "y_num = newsgroups_train.target\n",
    "X = pd.DataFrame(X_sparse.todense(), columns=vectorizer.get_feature_names())\n",
    "y = np.array(cats)[y_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 --- [5 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an initial inspection of the data, X and y (no more than 5 lines of code). Below the code, describe what the data are i.e. what the objects are, and what they represent (fewer than 4 sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical information:\n",
      "                abc      ability         able           ac       accept  \\\n",
      "count  2845.000000  2845.000000  2845.000000  2845.000000  2845.000000   \n",
      "mean      0.002436     0.002149     0.006709     0.001454     0.002286   \n",
      "std       0.030224     0.020739     0.034631     0.023100     0.021634   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.723914     0.420916     0.431380     0.519765     0.443354   \n",
      "\n",
      "            access    according     acquired     actually      adaptec  \\\n",
      "count  2845.000000  2845.000000  2845.000000  2845.000000  2845.000000   \n",
      "mean      0.003953     0.003115     0.000921     0.008366     0.001816   \n",
      "std       0.030111     0.026958     0.016275     0.043921     0.026026   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.663808     0.572124     0.413233     1.000000     0.578282   \n",
      "\n",
      "          ...           written        wrong      yankees         yeah  \\\n",
      "count     ...       2845.000000  2845.000000  2845.000000  2845.000000   \n",
      "mean      ...          0.002612     0.006811     0.002470     0.003564   \n",
      "std       ...          0.026988     0.039172     0.032786     0.031238   \n",
      "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "75%       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "max       ...          0.723081     0.568146     0.878185     0.693531   \n",
      "\n",
      "              year        years          yes         york        young  \\\n",
      "count  2845.000000  2845.000000  2845.000000  2845.000000  2845.000000   \n",
      "mean      0.016934     0.008797     0.006381     0.002772     0.003288   \n",
      "std       0.061181     0.039346     0.037319     0.025381     0.030929   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.650649     0.455493     0.646447     0.572455     0.703771   \n",
      "\n",
      "              zone  \n",
      "count  2845.000000  \n",
      "mean      0.001626  \n",
      "std       0.022175  \n",
      "min       0.000000  \n",
      "25%       0.000000  \n",
      "50%       0.000000  \n",
      "75%       0.000000  \n",
      "max       0.605576  \n",
      "\n",
      "[8 rows x 1000 columns]\n",
      "Possible class clusters:\n",
      " ['alt.atheism' 'comp.sys.ibm.pc.hardware' 'comp.sys.mac.hardware'\n",
      " 'rec.sport.baseball' 'rec.sport.hockey']\n",
      "Words:\n",
      " ['abc' 'ability' 'able' 'ac' 'accept' 'access' 'according' 'acquired'\n",
      " 'actually' 'adaptec' 'adapter' 'add' 'address' 'advance' 'advantage'\n",
      " 'advice' 'ago' 'agree' 'ahead' 'ahl' 'al' 'allow' 'allowed' 'allows'\n",
      " 'alomar' 'alt' 'american' 'amour' 'andrew' 'angeles' 'answer' 'answers'\n",
      " 'anybody' 'apparently' 'appears' 'apple' 'appreciate' 'appreciated' 'apr'\n",
      " 'april' 'area' 'aren' 'arena' 'argument' 'arguments' 'article' 'ask'\n",
      " 'asked' 'assume' 'assuming' 'atheism' 'atheist' 'atheists' 'atlanta'\n",
      " 'available' 'average' 'away' 'backup' 'bad' 'ball' 'baltimore' 'base'\n",
      " 'baseball' 'based' 'basically' 'basis' 'bat' 'bay' 'beat' 'belief'\n",
      " 'beliefs' 'believe' 'best' 'better' 'bible' 'big' 'biggest' 'bios' 'bit'\n",
      " 'black' 'blue' 'blues' 'board' 'boards' 'bob' 'bobby' 'book' 'boot' 'bos'\n",
      " 'boston' 'bought' 'box' 'braves' 'brian' 'brind' 'bring' 'brown' 'bruins'\n",
      " 'btw' 'buf' 'buffalo' 'built' 'bus' 'business' 'buy' 'buying' 'ca' 'cable'\n",
      " 'cache' 'cal' 'calgary' 'called' 'came' 'canada' 'canadian' 'caps'\n",
      " 'captain' 'card' 'cards' 'care' 'career' 'case' 'cause' 'cd' 'center'\n",
      " 'centris' 'certain' 'certainly' 'chance' 'change' 'changed' 'channel'\n",
      " 'cheap' 'check' 'cheers' 'chi' 'chicago' 'chip' 'chips' 'choice'\n",
      " 'christian' 'christianity' 'christians' 'city' 'claim' 'claims' 'clark'\n",
      " 'clear' 'clearly' 'clemens' 'cleveland' 'clock' 'close' 'club' 'coach'\n",
      " 'code' 'college' 'color' 'colorado' 'colors' 'com' 'come' 'comes' 'coming'\n",
      " 'comment' 'comments' 'common' 'comp' 'compare' 'compared' 'compatible'\n",
      " 'complete' 'completely' 'computer' 'computers' 'conclusion' 'conference'\n",
      " 'configuration' 'connect' 'connector' 'consider' 'considered'\n",
      " 'considering' 'contact' 'context' 'control' 'controller' 'copy' 'correct'\n",
      " 'cost' 'couldn' 'couple' 'course' 'coverage' 'cpu' 'created' 'cs' 'cubs'\n",
      " 'cup' 'current' 'currently' 'cut' 'data' 'date' 'dave' 'david' 'day'\n",
      " 'days' 'dead' 'deal' 'death' 'decent' 'decided' 'defense' 'defenseman'\n",
      " 'defensive' 'define' 'definition' 'deleted' 'det' 'details' 'detroit'\n",
      " 'device' 'devices' 'devils' 'did' 'didn' 'difference' 'different' 'dineen'\n",
      " 'directly' 'discussion' 'disk' 'disks' 'display' 'division' 'dma'\n",
      " 'dodgers' 'does' 'doesn' 'doing' 'don' 'dos' 'double' 'doubt' 'draft'\n",
      " 'drive' 'driver' 'drivers' 'drives' 'duo' 'earlier' 'early' 'easily'\n",
      " 'east' 'easy' 'ed' 'edm' 'edmonton' 'edu' 'effect' 'eisa' 'email' 'end'\n",
      " 'entire' 'era' 'eric' 'error' 'errors' 'esdi' 'especially' 'espn'\n",
      " 'ethernet' 'event' 'evidence' 'exactly' 'example' 'excellent' 'exist'\n",
      " 'existence' 'exists' 'expansion' 'expect' 'expected' 'experience'\n",
      " 'explain' 'external' 'extra' 'face' 'fact' 'faith' 'fallacy' 'false' 'fan'\n",
      " 'fans' 'faq' 'far' 'fast' 'faster' 'fax' 'feature' 'features' 'feel'\n",
      " 'field' 'figure' 'file' 'files' 'final' 'finally' 'finals' 'fine'\n",
      " 'finland' 'fit' 'flames' 'floppy' 'florida' 'flyers' 'follow' 'following'\n",
      " 'forget' 'form' 'format' 'fpu' 'francis' 'frank' 'free' 'friend' 'ftp'\n",
      " 'future' 'game' 'games' 'gateway' 'gave' 'general' 'generally' 'germany'\n",
      " 'gets' 'getting' 'gilmour' 'given' 'gives' 'giving' 'gm' 'goal' 'goalie'\n",
      " 'goals' 'god' 'gods' 'goes' 'going' 'gone' 'good' 'got' 'graphics' 'great'\n",
      " 'greatly' 'gretzky' 'group' 'guess' 'guy' 'guys' 'half' 'hall' 'hand'\n",
      " 'happen' 'happened' 'happens' 'happy' 'har' 'hard' 'hardware' 'hartford'\n",
      " 'haven' 'having' 'hawks' 'hd' 'head' 'heads' 'hear' 'heard' 'hell' 'hello'\n",
      " 'help' 'hi' 'high' 'higher' 'history' 'hit' 'hits' 'hitter' 'hitting'\n",
      " 'hockey' 'hold' 'home' 'hope' 'hot' 'houston' 'hp' 'human' 'ibm' 'ice'\n",
      " 'id' 'ide' 'idea' 'ideas' 'ii' 'iii' 'iisi' 'important' 'include'\n",
      " 'included' 'includes' 'including' 'individual' 'info' 'information'\n",
      " 'injury' 'install' 'installed' 'instead' 'interested' 'interesting'\n",
      " 'interface' 'internal' 'internet' 'irq' 'isa' 'islam' 'islamic'\n",
      " 'islanders' 'isn' 'issue' 'jagr' 'jeff' 'jersey' 'jesus' 'jets' 'jewish'\n",
      " 'jim' 'job' 'joe' 'john' 'jose' 'joseph' 'jumper' 'jumpers' 'just' 'keith'\n",
      " 'key' 'keyboard' 'kill' 'kind' 'king' 'kings' 'know' 'knowledge' 'known'\n",
      " 'knows' 'la' 'lack' 'large' 'late' 'later' 'law' 'lc' 'lead' 'leads'\n",
      " 'leafs' 'league' 'leave' 'led' 'left' 'lemieux' 'let' 'level' 'life'\n",
      " 'light' 'like' 'likely' 'limited' 'lindros' 'line' 'lines' 'list' 'little'\n",
      " 'live' 'll' 'local' 'logic' 'long' 'longer' 'look' 'looked' 'looking'\n",
      " 'looks' 'lopez' 'los' 'lose' 'loss' 'lost' 'lot' 'lots' 'louis' 'love'\n",
      " 'low' 'lower' 'luck' 'mac' 'machine' 'machines' 'macintosh' 'macs' 'mail'\n",
      " 'main' 'maine' 'major' 'majority' 'make' 'makes' 'making' 'man' 'manager'\n",
      " 'manual' 'maple' 'mark' 'master' 'matter' 'matthew' 'maybe' 'mb' 'mean'\n",
      " 'meaning' 'means' 'media' 'meg' 'memory' 'men' 'mention' 'mentioned'\n",
      " 'message' 'mets' 'mhz' 'michael' 'middle' 'mike' 'million' 'milwaukee'\n",
      " 'min' 'mind' 'minnesota' 'minor' 'minutes' 'missed' 'mode' 'model' 'modem'\n",
      " 'mon' 'moncton' 'money' 'monitor' 'monitors' 'months' 'montreal' 'moral'\n",
      " 'morality' 'morris' 'motherboard' 'mouse' 'murphy' 'murray' 'mvp' 'names'\n",
      " 'national' 'natural' 'nature' 'near' 'nec' 'necessary' 'need' 'needed'\n",
      " 'needs' 'net' 'network' 'new' 'news' 'newsgroup' 'nhl' 'nice' 'night' 'nj'\n",
      " 'nl' 'non' 'north' 'note' 'notice' 'noticed' 'nubus' 'number' 'numbers'\n",
      " 'ny' 'nyi' 'nyr' 'objective' 'obvious' 'obviously' 'offense' 'offensive'\n",
      " 'oh' 'oilers' 'ok' 'okay' 'old' 'ones' 'open' 'opinion' 'option' 'order'\n",
      " 'original' 'os' 'ott' 'ottawa' 'park' 'particular' 'particularly' 'pass'\n",
      " 'past' 'patrick' 'paul' 'pay' 'pc' 'pds' 'penalties' 'penalty' 'penguins'\n",
      " 'pens' 'people' 'perfect' 'performance' 'period' 'person' 'personal'\n",
      " 'peter' 'phi' 'philadelphia' 'phillies' 'phone' 'pick' 'pin' 'pins'\n",
      " 'pirates' 'pit' 'pitch' 'pitcher' 'pitchers' 'pitching' 'pittsburgh'\n",
      " 'place' 'play' 'played' 'player' 'players' 'playing' 'playoff' 'playoffs'\n",
      " 'plays' 'plug' 'plus' 'point' 'points' 'poor' 'port' 'ports' 'position'\n",
      " 'possible' 'possibly' 'post' 'posted' 'posting' 'posts' 'power'\n",
      " 'powerbook' 'pp' 'present' 'president' 'press' 'pretty' 'price' 'print'\n",
      " 'printer' 'pro' 'probably' 'problem' 'problems' 'processor' 'program'\n",
      " 'programs' 'prove' 'provide' 'ps' 'pt' 'pts' 'puck' 'purpose' 'quadra'\n",
      " 'quality' 'quantum' 'que' 'quebec' 'question' 'questions' 'quick' 'quite'\n",
      " 'qur' 'radio' 'ram' 'rangers' 'rate' 'rbi' 'read' 'reading' 'real'\n",
      " 'really' 'reason' 'reasons' 'recchi' 'received' 'recent' 'recently'\n",
      " 'record' 'red' 'reds' 'reference' 'regular' 'religion' 'religions'\n",
      " 'religious' 'remember' 'remove' 'replace' 'reply' 'report' 'require'\n",
      " 'requires' 'response' 'rest' 'result' 'results' 'return' 'right' 'roger'\n",
      " 'rom' 'ron' 'rookie' 'round' 'roy' 'rule' 'rules' 'run' 'running' 'runs'\n",
      " 'sabres' 'said' 'san' 'sanderson' 'save' 'saves' 'saw' 'say' 'saying'\n",
      " 'says' 'science' 'score' 'scored' 'scorer' 'scores' 'scoring' 'scott'\n",
      " 'screen' 'scsi' 'se' 'season' 'sec' 'second' 'seconds' 'section' 'seen'\n",
      " 'self' 'sell' 'send' 'sense' 'sent' 'serial' 'series' 'service' 'set'\n",
      " 'settings' 'setup' 'sex' 'sharks' 'short' 'shot' 'shots' 'shouldn' 'shown'\n",
      " 'sign' 'similar' 'simm' 'simms' 'simple' 'simply' 'single' 'situation'\n",
      " 'size' 'sj' 'slave' 'slot' 'slots' 'slow' 'small' 'smith' 'society'\n",
      " 'soderstrom' 'software' 'solution' 'somebody' 'soon' 'sorry' 'sort'\n",
      " 'sound' 'sounds' 'sox' 'special' 'speed' 'sports' 'st' 'stadium' 'staff'\n",
      " 'standard' 'standings' 'stanley' 'star' 'stars' 'start' 'started'\n",
      " 'starting' 'starts' 'state' 'statement' 'statements' 'states' 'stats'\n",
      " 'stay' 'steve' 'stick' 'stl' 'stop' 'story' 'straight' 'strong' 'stuff'\n",
      " 'subject' 'suck' 'sunday' 'supply' 'support' 'supports' 'supposed' 'sure'\n",
      " 'sweden' 'switch' 'sys' 'systems' 'taken' 'takes' 'taking' 'talent' 'talk'\n",
      " 'talking' 'tampa' 'tape' 'tb' 'team' 'teams' 'tech' 'technical'\n",
      " 'technology' 'tell' 'tells' 'terms' 'test' 'text' 'thank' 'thanks' 'thing'\n",
      " 'things' 'think' 'thinking' 'thomas' 'thought' 'thread' 'tie' 'time'\n",
      " 'times' 'today' 'told' 'tom' 'took' 'tor' 'toronto' 'total' 'totally'\n",
      " 'trade' 'traded' 'transfer' 'tried' 'trouble' 'true' 'truth' 'try'\n",
      " 'trying' 'turn' 'tv' 'type' 'uk' 'understand' 'unfortunately' 'universe'\n",
      " 'university' 'unless' 'upgrade' 'usa' 'use' 'used' 'uses' 'using'\n",
      " 'usually' 'valid' 'value' 'van' 'vancouver' 'various' 've' 'version' 'vga'\n",
      " 'video' 'view' 'vlb' 'vram' 'vs' 'wait' 'want' 'wanted' 'wants' 'war'\n",
      " 'washington' 'wasn' 'watch' 'watching' 'way' 'weak' 'week' 'weeks' 'went'\n",
      " 'west' 'western' 'white' 'wide' 'willing' 'win' 'windows' 'wings' 'winner'\n",
      " 'winning' 'winnipeg' 'wins' 'wish' 'women' 'won' 'wonder' 'wondering'\n",
      " 'word' 'words' 'work' 'working' 'works' 'world' 'worse' 'worth' 'wouldn'\n",
      " 'write' 'written' 'wrong' 'yankees' 'yeah' 'year' 'years' 'yes' 'york'\n",
      " 'young' 'zone']\n",
      "Data    abc   ability  able   ac  accept  access  according  acquired  actually  \\\n",
      "0  0.0  0.000000   0.0  0.0     0.0     0.0   0.000000       0.0       0.0   \n",
      "1  0.0  0.000000   0.0  0.0     0.0     0.0   0.000000       0.0       0.0   \n",
      "2  0.0  0.000000   0.0  0.0     0.0     0.0   0.000000       0.0       0.0   \n",
      "3  0.0  0.000000   0.0  0.0     0.0     0.0   0.000000       0.0       0.0   \n",
      "4  0.0  0.000000   0.0  0.0     0.0     0.0   0.000000       0.0       0.0   \n",
      "5  0.0  0.000000   0.0  0.0     0.0     0.0   0.000000       0.0       0.0   \n",
      "6  0.0  0.000000   0.0  0.0     0.0     0.0   0.000000       0.0       0.0   \n",
      "7  0.0  0.000000   0.0  0.0     0.0     0.0   0.000000       0.0       0.0   \n",
      "8  0.0  0.000000   0.0  0.0     0.0     0.0   0.133553       0.0       0.0   \n",
      "9  0.0  0.420916   0.0  0.0     0.0     0.0   0.000000       0.0       0.0   \n",
      "\n",
      "   adaptec  ...   written  wrong  yankees  yeah  year  years  yes  york  \\\n",
      "0      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "1      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "2      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "3      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "4      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "5      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "6      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "7      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "8      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "9      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
      "\n",
      "   young  zone  \n",
      "0    0.0   0.0  \n",
      "1    0.0   0.0  \n",
      "2    0.0   0.0  \n",
      "3    0.0   0.0  \n",
      "4    0.0   0.0  \n",
      "5    0.0   0.0  \n",
      "6    0.0   0.0  \n",
      "7    0.0   0.0  \n",
      "8    0.0   0.0  \n",
      "9    0.0   0.0  \n",
      "\n",
      "[10 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "print(\"Statistical information:\\n\",X.describe())\n",
    "print(\"Possible class clusters:\\n\",np.unique(y))\n",
    "print(\"Words:\\n\",X.columns.values)\n",
    "print(\"Data\",X.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*\n",
    "As it is obvious from the table, the columns are so much sparse and there is a possibility of reducing the computational complexity by employing proper algorithms like (TACO from MIT, many more). This is re-ensured by the inter-quartile region values (25%, 50%, 75%) in the table. Surprisingly, even after todense() is applied, the data is sparse.\n",
    "\n",
    "Instead of frequency score or probability score, we use TF-IDF, which is a famous representation of words (recently, word2vec and Glove).\n",
    "\n",
    "[[ A high weight in tfâ€“idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Source: Wikipedia]]\n",
    "\n",
    "Y values have been trimmed to only 5 categories from 20 as it is in the actual dataset.\n",
    "\n",
    "Stopwords are intentionally removed as they will result in having more importance and they dont matter much with clustering (document classification from an application POV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 --- [2 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) object with 5 clusters. Make sure that you can reproduce your results exactly. *Hint: there is an argument for this*. You need only set two arguments; others can be kept as default. Call the instantiated object `kmeans`. Use the `fit()` method to fit to the training data (X imported above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=1337, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "km = KMeans(n_clusters=5, random_state=1337, init = \"k-means++\") # init is used just to speed up the convergence\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.3 --- [6 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evauate the quality of this fit.\n",
    "\n",
    "1. Get a feel for the average distance of a data point from the centre: print the mean of the euclidean distances of all data points from the mean data point (in the whole dataset)\n",
    "1. print the inertia of the model. *Hint: check the properties of the kmeans object*\n",
    "1. print the adjusted rand index of the model. *Hint: `adjusted_rand_score`*\n",
    "\n",
    "Below the code: \n",
    "1. Define what the inertia and adjusted rand score are (one or two sentences). *Hint: check [sklearn documentation](http://scikit-learn.org/stable/modules/clustering.html)*\n",
    "1. Comment on the quality of the clustering implied by the adjusted rand score and inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average distance of a data point from the centre:  0.954584949852\n",
      "Total Inertia:  2593.37189411\n",
      "ARI score:  0.250608658757\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "# np.linalg.norm(X.mean-X.iloc[1])  # try-out\n",
    "print(\"average distance of a data point from the centre: \",np.mean(np.linalg.norm(X-X.mean(), axis=1)))\n",
    "print(\"Total Inertia: \",km.inertia_)\n",
    "print(\"ARI score: \", adjusted_rand_score(y, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*\n",
    "1. Inertia:\n",
    "Inertia is basically euclidean distance between a data point to its nearest centroid (or mean of the cluster). Here, we get a sum of the distance between all te datapoints and their respective cluster means. Going deeper, inertia cannot be used as an objective metric, inertia is sensitive to the number of datapoints, number of clusters. It can only be a subjective measure. Fundamentally, k-means is all about minimizing inertia and thats its convergence. It yields 0 for farther points and 1 for 0 distance. An observation is total inertia value will always be lesser than the total number of datapoints.\n",
    "\n",
    "ARI:\n",
    "ARI is a simple measure that depicts the agreement of the classes on a single datapoint. It does not concern the permutation of the classes for a single class and spend k! computations on it. It applies the algorithm on a contigency table where pair-wise similairity is measured. ARI score will be 0 for completely orthogonal datapoints and 1 for same datapoints.\n",
    "\n",
    "2. Quality of the classifier seems to be poor (atleast as depicted by inertia and ARI). Given a data size of 2845 our inertia is 2593 which shows most datapoints are far away from their centroids. Consequently the clusters becoming spread all over. ARI gives 0.25 (25%), where even prior probability is 20% (100/5 clusters). ARI mirrors the poor quality of the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.4 --- [3 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print:\n",
    "1. the number of datapoints with each label\n",
    "2. the number of datapoints assigned to each cluster. *Hint: you should use the properties of the kmeans object you just fit.* \n",
    "\n",
    "Below the code, comment on the distribution of datapoints to cluster centres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of datapoints assigned to each cluste (array([0, 1, 2, 3, 4], dtype=int32), array([969, 689, 222, 381, 584]))\n",
      "the number of datapoints assigned to each cluste (array(['alt.atheism', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
      "       'rec.sport.baseball', 'rec.sport.hockey'],\n",
      "      dtype='<U24'), array([480, 590, 578, 597, 600]))\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "print(\"the number of datapoints assigned to each cluster\",np.unique(km.labels_, return_counts=True))\n",
    "print(\"the number of datapoints assigned to each cluster\",np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.5 --- [3 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't visualise these cluster centres directly, since they are 1000 dimensional. However, we can at least measure the distance between each centre. Create a distance matrix such that the entry with index (i,j) shows the distance between centre i and j. *Hint: again you should use the properties of the kmeans object you just fit.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.6 --- [3 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster centre label, plot the counts of the true labels. The cluster labels are a property of the k-means object, the true labels are contained in `y`. Make sure that you label the plot axes and legend clearly. Below the code, comment on the quality of the fit. *Hint: you can do the main plot (without labels) in one line with seaborn (you're free to do it as you like though!).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.7 --- [8 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now investigate using dimensionality reduction to try and improve the quality of the fit. Use the sklearn implementation of [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and its method `fit_transform()` to create a reduced reduced dataset for `n_components` = [1,2,3,5,10,20,50,100,500,1000] i.e. create datasets that are of shape `(N, d)` for `d` in `n_components`. Fit k-means to each reduced dataset and report the `inertia` and `adjusted_rand_score` for each iteration.\n",
    "\n",
    "Plot `adjusted_rand_score` against number of principal components (label graph). Use a log scale on the x axis. Below the graph:\n",
    "1. describe what it shows\n",
    "1. explain why we cannot use inertia to choose the best number of principal components\n",
    "1. explain why dimensionality reduction could help k-means perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.8 --- [6 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. So we can visualise the data, create X_2d: the dataset X transformed down to 2 principal component dimensions. Use sklearn's implementation of [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and its method `fit_transform()` to do this (as above). \n",
    "\n",
    "1. Create a new k-means object, `kmeans_pca`, and fit the 2d data to it. Show the adjusted rand score.\n",
    "\n",
    "1. As above, for each cluster centre label, plot the counts of the true labels. The cluster labels are a property of the k-means object, the true labels are contained in `y`. Make sure that you label the plot axes and legend clearly. Print below it the number of data points each cluster is responsible for.\n",
    "\n",
    "1. Finally, below the plot, comment on the difference between these clusters and the clusters on the 1000 dimensional data with respect to the distribution of the labels in each. Are they better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.9 --- [1 mark] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above plot, apply a label to each cluster centre. Create a new vector, `labels_interp`, which is the same as `kmeans.labels_`, but instead of numbers, the interpreted label of the cluster centre. For example, if you decided cluster 0 was 'apples', 1 was 'pears', 2 was 'stairs', and `kmeans.labels_` was `[2, 0, 1, 1, 0]`, create  `labels_interp = ['stairs', 'apples', 'pears', 'pears', 'apples']`. Hint: an example of how to do this is given in the last line of Question 1.0.\n",
    "\n",
    "**N.B. be careful to use the `kmeans_pca` object you created above, not the first kmeans object you made**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.10 --- [3 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a normalised (by true label) confusion matrix of your interpreted labels from the k-means clustering and the true labels. As usual, you may use any functions from previous assignments or labs. Clearly label the axes of the plot. Check that these confusions correlate with your expectations! N.B. this is just a slightly different way of interpreting the information in the count plot above (focussed this time on the true labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.11 --- [6 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are now in 2 dimensions, let's visualise the data, the cluster centres, and the decision boundaries via a [Voronoi_diagram](https://en.wikipedia.org/wiki/Voronoi_diagram). You'll essentially be able to copy and paste the code from the [sklearn kmeans digits example](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html) and edit a few bits to get over half marks.\n",
    "\n",
    "Clearly mark each cluster centre.\n",
    "\n",
    "For full marks, additionally:\n",
    "* label each cluster centre with the inferred cluster label\n",
    "* create a second plot which clearly shows where the true classes lie within the pca space e.g. the [sklearn PCA example here](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.12 --- [4 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write a **1 or 2 sentence** answer for each question*.\n",
    "\n",
    "1. Is the kmeans algorithm deterministic?\n",
    "1. Did the algorithm you ran above in Question 1.2 (fitting k-means) do multiple initialisations? If not, explain why it was not necessary. If so, how was the final model selected?\n",
    "1. The default method for initialising centres in the sklearn implementation is [kmeans++](https://en.wikipedia.org/wiki/K-means%2B%2B). Name another method for initialising and a problem with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA with MNIST Data [50%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the second part of the assignment we will explore the MNIST digits dataset. We expect the digits to lie in a lower-dimensional manifold and want to examine the representation we get by applying Principal Components Analysis (PCA). PCA maps the data into a new space by effectively rotating the base vectors of the input space to the directions with the highest variance. We will assess the impact of this mapping to the classification task and the separability of the data in the PCA space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.0 --- [0 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to load the digits object and print its description.\n",
    "\n",
    "**Do not change any of the code in this question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.1 --- [8 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you execute `digits.keys()`, you'll see this object contains the data, the targets (the labels), and the images (the data reshaped to 8x8 images). More typically the mnist data are [28x28 images](http://yann.lecun.com/exdb/mnist/), but this is a preprocessed version. \n",
    "\n",
    "1. Use the pandas describe method on the data to get a feel for the range of each dimension\n",
    "1. What are the max/min values for each dimension?\n",
    "1. Extract the standard deviations from the output of the describe method (just another DataFrame with 'std' as one of the index values), reshape to an 8x8 image, and plot a heatmap (use `sns.heatmap()`) to show you which dimensions vary the most. For a bonus mark, produce a plot like this for each digit. *Hint: you can use `.groupby(digits.target)` before calling `.describe()`*.\n",
    "1. Use `sns.heatmap()` to plot the first 9 digits in the dataset\n",
    "\n",
    "Below the plots, answer this question in a markdown cell:\n",
    "1. Are all of the dimensions going to be equally helpful for modelling? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.2 --- [10 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new `PCA` object with `n_components = digits.data.shape[1]`. Plot the explained variance **ratio** against the number of components. You may find [this example](http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html#sphx-glr-auto-examples-plot-digits-pipe-py) quite handy...\n",
    "\n",
    "Find the point where 95% of the variance has been explained. Use `plt.vlines()` to add a dotted verical line to the graph at that point and use `plt.annotate()` to label the number of eigenvectors used to explain that variance.\n",
    "\n",
    "Below the plot, explain what you observe. What does this suggest about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.3 --- [8 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find the eigenvectors stored within `pca.components_`. Reuse your code from Question 2.1 and plot the first 9 principal components (PCs). Below, plot `pca.mean_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.4 --- [10 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make clear how these eigenvectors and the mean are used to approximate the data. `pca.transform(digits.data[idx,:].reshape(1, -1)).flatten()` will transform the digit in row `idx` into pca space. Another way to think about this is that it will give you the coefficents to multiply each eigenvector by and to add to the mean digit such that you can reconstruct the digit.\n",
    "\n",
    "For the digit with idx = 0 (which should itself be a zero), create 4 plots:\n",
    "1. The original digit\n",
    "1. The digit reconstructed using 1 principal component\n",
    "1. The digit reconstructed using 2 principal components\n",
    "1. The digit reconstructed using 5 principal components\n",
    "\n",
    "In the plot titles show:\n",
    "1. the number of principal components used\n",
    "1. the percentage of variance explained by that number of principal components\n",
    "1. the coefficients of each principal component rounded to nearest integer (tip, convert to integers to reduce print space), i.e. the PCA space vector.\n",
    "\n",
    "Below the plots, comment on the result. Do the eigenvectors produce realistic data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.5 --- [14 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge question** \n",
    "\n",
    "Fit 4 models:\n",
    "1. SVC with a linear kernel, no shrinking, and a random_state on digit data\n",
    "1. SVC with a linear kernel, no shrinking, and the same random_state on pca transformed digit data (use the full 64 component pca above)\n",
    "1. Gaussian Naive Bayes Classifier on digit data\n",
    "1. Gaussian Naive Bayes Classifier on pca transformed digit data (use the full 64 component pca above)\n",
    "\n",
    "Use 5 fold cross validation and take the mean fold score as the result. Plot or print the results.\n",
    "\n",
    "Below the code, explain why one classifier improved when we used PCA, but the other did not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
